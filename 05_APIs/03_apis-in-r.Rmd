---
title: 'Data Analysis in R: Lecture Notes'
author: "PS239T"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
    df_print: paged
    theme: flatly
    highlight: pygments
---

**********************************************************

# 0) GET API KEY

## 0.1. Exercise 1

Go to `https://developer.nytimes.com/get-started` and create an account for the NYT API. Then create an API key. 

Save API key in R and python scripts in the 'api_keys' subfolder. 


# 1) SET UP 

## 1.1. General 
```{r}
# remove all objects
rm(list=ls())

# Check working directory is "PS239T_Fall2019/05_APIs"
getwd()
```

## 1.2. Packages
```{r}
# Unload all packages 
library(pacman)
pacman::p_unload(all)

# Add packages 
pacman::p_load(
  tidyverse, #dplyr, readr, etc.
  magrittr #%<>% operator
)
```

If pacman isn't installed, try the following code (will need to manually run).
```{r, eval=FALSE}
# Install pacman if necessary (uncomment to do so)
install.packages("pacman")

# Alternate installation
install.packages("pacman", "devtools")
library(devtools)
install_github("trinker/pacman")
```

## 1.3. Load API keys 

I have saved my API keys in the subfolder 'api_keys_jbc', but you don't see that folder in my github repo. That is because I added this subfolder the the .gitignore file. 
```{r}
# Detmine if running on Julia's computer
julia_computer = (getwd()=="C:/Users/Julia/OneDrive/Documents/Berkeley/2019-08_Fall/PS239T_Fall2019/05_APIs")

# If so, read API keys from her subfolder (hidden from you b/c it's listed in the .gitignore file)
if(julia_computer==TRUE) source("api_keys_jbc/01_api_keys_nyt_r.R")
```

You should run the following code instead after saving your API key in the 'api_keys' subfolder:
```{r}
if(julia_computer==FALSE) source("api_keys/01_api_keys_nyt_r.R")
```


**********************************************************

# 2) API EXAMPLE

## 2.1. Constructing API GET Requests in R

Because using Web APIs in R will involve repeatedly constructing different GET requests with slightly different components each time, it is helpful to store many of the individuals components as objects and combine them using ```paste()``` when ready to send the request.

In the first place, we know that every call will require us to provide a) a base URL for the API, b) some authorization code or key, and c) a format for the response.

```{r}
# Create objects holding the key, base url, and response format
key<-nyt_key_1
base.url<-"http://api.nytimes.com/svc/search/v2/articlesearch"
response.format<-".json"
```

Secondly, we need to specify our search terms, along with any filters to be placed on the results.  In this case, we are searching for the phrase "on fleek", though we specifically want it to appear in the body of the text.
```{r}
# Specify a main search term (q)
search.term<-"on fleek"

# Specify and encode filters (fc)
filter.query<-"body:\"on fleek\"" 
```

Note that it can often be tricky to properly re-format character strings stored in R objects to character strings suitable for GET requests.  For example, the filter above uses quotation marks to specify that we wanted to retrieve the phrase exactly.  But to include those quotation marks inside a character string that --- following R syntax --- must itself be surrounded by double quotation marks, these original characters need to be escaped with a backslash.  This results in the stored R string appearing to be different from the parsed R string.     
```{r}
# NOTE: double quotes within double quotes must be escaped with / so R can parse the character string
print(filter.query) # How R stores the string
cat(filter.query) # How R parses the string
```

To overcome some of these encoding issues, it is often helpful to URL encode our strings.  URL encoding basically translates punctuation marks, white space, and other non alphanumeric characters into a series of unique characters only recognizeable by URL decoders.  If you've ever seen %20 in a URL, this is actually a placeholder for a single space.  R provides helpful functions to doing this translation automatically.  
```{r}
# URL-encode the search and its filters
search.term<-URLencode(URL = search.term, reserved = TRUE)
filter.query<-URLencode(URL = filter.query, reserved = TRUE)
print(search.term)
print(filter.query)
```

Once all the pieces of our GET request are in place, we can use either the ```paste()``` or ```paste0()``` to combine a number of different character strings into a single character string.  This final string will be our URL for the GET request.
```{r}
# Paste components together to create URL for get request
get.request<-paste0(base.url, response.format, "?", "q=", search.term, "&fq=", filter.query, "&api-key=", key)
print(get.request)
```

## 2.2. Send GET request 

Once we have the URL complete, we can send a properly formated GET request.  There are several packages that can do this, but ***httr*** provides a good balance of simplicity and reliability.  The main function of interest here is ```GET()```:
```{r}
# Send the GET request using httr package
response<-httr::GET(url = get.request)
print(response)
```

## 2.3. Extract html response 

The ```content()``` function allows us to extract the html response in a format of our choosing (raw text, in this case):
```{r} 
# Inspect the content of the response, parsing the result as text
response<-httr::content(x = response, as = "text")
substr(x = response, start = 1, stop = 1000)
```

## 2.4. Convert results 

The final step in the process involves converting the results from JSON format to something easier to work with -- notably a data.frame.  The ***jsonlite*** package provides several easy conversion functions for moving between JSON and vectors, data.frames, and lists.
```{r}
# Convert JSON response to a dataframe
response.df<-jsonlite::fromJSON(txt = response, simplifyDataFrame = TRUE, flatten = TRUE)

# Inspect the dataframe
str(response.df, max.level = 3)

# Get number of hits
print(response.df$response$meta$hits)
```

## 2.5. Functionalize GET Request 

Of course, most experiences using Web APIs will require *multiple* GET requests, each different from the next.  To speed this process along, we can create a function that can take several arguments and then automatically generate a properly formated GET request URL.  Here, for instance, is one such function we might write:
```{r}
# Write a function to create get requests
nytapi<-function(search.terms=NULL, begin.date=NULL, end.date=NULL, page=NULL,
                 base.url="http://api.nytimes.com/svc/search/v2/articlesearch",
                 response.format=".json",
                 key){

  # Combine parameters
  params<-list(
    c("q", search.terms),
    c("begin_date", begin.date),
    c("end_date", end.date),
    c("page", page)
  )
  params<-params[sapply(X = params, length)>1]
  params<-sapply(X = params, FUN = paste0, collapse="=")
  params<-paste0(params, collapse="&")
  
  # URL encode query portion
  query<-URLencode(URL = params, reserved = FALSE)

  # Combine with base url and other options
  get.request<-paste0(base.url, response.format, "?", query, "&api-key=", key)
  
  # Send GET request
  response<-httr::GET(url = get.request)
  
  # Parse response to JSON
  response<-httr::content(response, "text")  
  response<-jsonlite::fromJSON(txt = response, simplifyDataFrame = T, flatten = T)
  
  return(response)
}
```

## 2.6. Loop through pages 

Now that we have our handy NYT API function, let's try and do some data analysis.  To figure out whether impeachment has been "trending", we can start by using our handy function to get a count of how often the New York Times mentions it...
 
```{r}
# Get number of hits, number of page queries
impeachment<-nytapi(search.terms = "impeachment", begin.date = 201909010, end.date = 20190930, key=nyt_key_1)
hits<-impeachment$response$meta$hits
print(hits)
round(hits/10)
```

After making a quick call to the API, it appears that we have 454 hits. Since the API only allows us to download 10 results at a time, we need to make 46 calls! 
```{r, eval=FALSE}
# Get all articles   
impeachment.articles<-sapply(X = 0:46, FUN = function(page){
  #cat(page, "")
  response<-tryCatch(expr = {
    r<-nytapi(search.terms = "impeachment", begin.date = 20190901, end.date = 20190930, page = page, key = nyt_key_1)
    r$response$docs
  }, error=function(e) NULL)
  Sys.sleep(7) #sleep 7 seconds
  print(paste0("Finished page ",page,"."))
  return(response)
})

impeachment.articles.orig <- impeachment.articles

# Combine list of dataframes
impeachment.articles<-impeachment.articles[!sapply(X = impeachment.articles, FUN = is.null)]
impeachment.articles<-dplyr::bind_rows(impeachment.articles)

# save
save(impeachment.articles, impeachment.articles.orig, file="data_raw/impeachment_articles.RData")
```

Load 
```{r}
load("data_raw/impeachment_articles.RData")
```

## 2.7. Analyze Results

To figure out how impeachment's popularity is changing over time, all we need to do is add an indicator for the year and month each article was published in.
```{r}
# Add year-month indicators
impeachment.articles$year.month<-format(as.Date(impeachment.articles$pub_date), "%Y-%m")

# Count articles per month
impeachment.permonth <-impeachment.articles %>%
  group_by(year.month) %>%
  summarise(monthly.articles=n())

# Plot the trend over time
ggplot(data = impeachment.permonth, 
       aes(x = pub_date, y = monthly.articles)) +
  geom_point() +
  theme_bw() + 
  xlab(label = "Date") +
  ylab(label = "Article Count") +
  ggtitle(label = "Coverage of Impeachment")
```


## 2.8. Exercise 2

What if we take a look at coverage of something else...

```{r, eval=FALSE}
st <- "ADD SEARCH TERMS"
start<-"ADD DATE" 
end<-"ADD DATE"

# Get number of hits, number of page queries
res<-nytapi(search.terms = "______________", begin.date = ________, end.date = ________)
hits<-res$response$meta$hits
print(hits)
round(hits/10)

# Get all articles   
res.articles<-sapply(X = 0:660, FUN = function(page){
  #cat(page, "")
  response<-tryCatch(expr = {
    r<-nytapi(search.terms = "______________", begin.date = ________, end.date = ________, page = page)
    r$response$docs
    Sys.sleep(10) #sleep 10 seconds
  }, error=function(e) NULL)
  return(response)
})

# Combine list of dataframes
res.articles<-res.articles[!sapply(X = res.articles, FUN = is.null)]
res.articles<-dplyr::bind_rows(res.articles)

# Add year-month indicators
res.articles$year.month<-format(as.Date(res.articles$pub_date), "%Y-%m")

# Count articles per month
res.permonth<-res.articles %>%
  group_by(year.month) %>%
  summarise(monthly.articles=n())

# Plot the trend over time
ggplot(data = res.permonth, 
       aes(x = year.month, y = monthly.articles)) +
  geom_point() +
  theme_bw() + 
  xlab(label = "Date") +
  ylab(label = "Article Count") +
  ggtitle(label = "ADD TILE")
```

**********************************************************

# 3) PACKAGES

For most popular APIs, the R community has created packages that format, send, and extract data from GET requests. 

## (Optional) Exercise 3:

Replicate the above analysis of impeachment stories in part 2 using the `rtimes` package. 

```{r, eval=FALSE}
pacman::p_load(
  rtimes #https://cran.r-project.org/web/packages/rtimes/vignettes/rtimes_vignette.html
)
```







